{"Video Generation": {"2508.11484": "|**2025-08-15**|**CineTrans: Learning to Generate Videos with Cinematic Transitions via Masked Diffusion Models**|Xiaoxue Wu et.al.|[2508.11484](http://arxiv.org/abs/2508.11484)|**[link](https://github.com/UknowSth/CineTrans)**|\n", "2508.11183": "|**2025-08-15**|**Versatile Video Tokenization with Generative 2D Gaussian Splatting**|Zhenghao Chen et.al.|[2508.11183](http://arxiv.org/abs/2508.11183)|null|\n", "2508.11049": "- 2025-08-14, **GenFlowRL: Shaping Rewards with Generative Object-Centric Flow in Visual Reinforcement Learning**, Kelin Yu et.al., Paper: [http://arxiv.org/abs/2508.11049](http://arxiv.org/abs/2508.11049)\n", "2508.10858": "|**2025-08-14**|**Hierarchical Fine-grained Preference Optimization for Physically Plausible Video Generation**|Harold Haodong Chen et.al.|[2508.10858](http://arxiv.org/abs/2508.10858)|null|\n", "2508.09822": "- 2025-08-13, **Physical Autoregressive Model for Robotic Manipulation without Action Pretraining**, Zijian Song et.al., Paper: [http://arxiv.org/abs/2508.09822](http://arxiv.org/abs/2508.09822)\n", "2508.09667": "|**2025-08-13**|**GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors**|Xingyilang Yin et.al.|[2508.09667](http://arxiv.org/abs/2508.09667)|null|\n", "2508.09632": "- 2025-08-15, **Preacher: Paper-to-Video Agentic System**, Jingwei Liu et.al., Paper: [http://arxiv.org/abs/2508.09632](http://arxiv.org/abs/2508.09632)\n", "2508.09476": "- 2025-08-14, **From Large Angles to Consistent Faces: Identity-Preserving Video Generation via Mixture of Facial Experts**, Yuji Wang et.al., Paper: [http://arxiv.org/abs/2508.09476](http://arxiv.org/abs/2508.09476), Code: **[https://github.com/rain152/LFA-Video-Generation](https://github.com/rain152/LFA-Video-Generation)**\n", "2508.09383": "- 2025-08-12, **X-UniMotion: Animating Human Images with Expressive, Unified and Identity-Agnostic Motion Latents**, Guoxian Song et.al., Paper: [http://arxiv.org/abs/2508.09383](http://arxiv.org/abs/2508.09383)\n", "2508.09136": "- 2025-08-12, **Turbo-VAED: Fast and Stable Transfer of Video-VAEs to Mobile Devices**, Ya Zou et.al., Paper: [http://arxiv.org/abs/2508.09136](http://arxiv.org/abs/2508.09136)\n", "2508.08978": "|**2025-08-12**|**TaoCache: Structure-Maintained Video Generation Acceleration**|Zhentao Fan et.al.|[2508.08978](http://arxiv.org/abs/2508.08978)|null|\n", "2508.08601": "|**2025-08-14**|**Yan: Foundational Interactive Video Generation**|Deheng Ye et.al.|[2508.08601](http://arxiv.org/abs/2508.08601)|null|\n", "2508.08086": "|**2025-08-11**|**Matrix-3D: Omnidirectional Explorable 3D World Generation**|Zhongqi Yang et.al.|[2508.08086](http://arxiv.org/abs/2508.08086)|null|\n", "2508.08048": "- 2025-08-11, **S^2VG: 3D Stereoscopic and Spatial Video Generation via Denoising Frame Matrix**, Peng Dai et.al., Paper: [http://arxiv.org/abs/2508.08048](http://arxiv.org/abs/2508.08048)\n", "2508.07981": "- 2025-08-12, **Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation**, Fangyuan Mao et.al., Paper: [http://arxiv.org/abs/2508.07981](http://arxiv.org/abs/2508.07981)\n", "2508.07905": "|**2025-08-11**|**Generative Video Matting**|Yongtao Ge et.al.|[2508.07905](http://arxiv.org/abs/2508.07905)|null|\n", "2508.07901": "- 2025-08-12, **Stand-In: A Lightweight and Plug-and-Play Identity Control for Video Generation**, Bowen Xue et.al., Paper: [http://arxiv.org/abs/2508.07901](http://arxiv.org/abs/2508.07901)\n", "2508.07769": "|**2025-08-11**|**Dream4D: Lifting Camera-Controlled I2V towards Spatiotemporally Consistent 4D Generation**|Xiaoyan Liu et.al.|[2508.07769](http://arxiv.org/abs/2508.07769)|null|\n", "2508.07557": "|**2025-08-11**|**Splat4D: Diffusion-Enhanced 4D Gaussian Splatting for Temporally and Spatially Consistent Content Creation**|Minghao Yin et.al.|[2508.07557](http://arxiv.org/abs/2508.07557)|null|\n", "2508.07149": "|**2025-08-10**|**SketchAnimator: Animate Sketch via Motion Customization of Text-to-Video Diffusion Models**|Ruolin Yang et.al.|[2508.07149](http://arxiv.org/abs/2508.07149)|null|\n", "2508.06902": "|**2025-08-09**|**eMotions: A Large-Scale Dataset and Audio-Visual Fusion Network for Emotion Analysis in Short-form Videos**|Xuecheng Wu et.al.|[2508.06902](http://arxiv.org/abs/2508.06902)|null|\n", "2508.06715": "- 2025-08-08, **Restage4D: Reanimating Deformable 3D Reconstruction from a Single Video**, Jixuan He et.al., Paper: [http://arxiv.org/abs/2508.06715](http://arxiv.org/abs/2508.06715)\n", "2508.06392": "|**2025-08-08**|**FVGen: Accelerating Novel-View Synthesis with Adversarial Video Diffusion Distillation**|Wenbin Teng et.al.|[2508.06392](http://arxiv.org/abs/2508.06392)|null|\n", "2508.06082": "- 2025-08-08, **SwiftVideo: A Unified Framework for Few-Step Video Generation through Trajectory-Distribution Alignment**, Yanxiao Sun et.al., Paper: [http://arxiv.org/abs/2508.06082](http://arxiv.org/abs/2508.06082)\n", "2508.05635": "|**2025-08-07**|**Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation**|Yue Liao et.al.|[2508.05635](http://arxiv.org/abs/2508.05635)|null|\n", "2508.04467": "|**2025-08-06**|**4DVD: Cascaded Dense-view Video Diffusion Model for High-quality 4D Content Generation**|Shuzhou Yang et.al.|[2508.04467](http://arxiv.org/abs/2508.04467)|null|\n", "2508.04016": "- 2025-08-07, **S$^2$Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation**, Weilun Feng et.al., Paper: [http://arxiv.org/abs/2508.04016](http://arxiv.org/abs/2508.04016)\n", "2508.03480": "|**2025-08-05**|**VideoGuard: Protecting Video Content from Unauthorized Editing**|Junjie Cao et.al.|[2508.03480](http://arxiv.org/abs/2508.03480)|null|\n", "2508.03334": "- 2025-08-06, **Macro-from-Micro Planning for High-Quality and Parallelized Autoregressive Long Video Generation**, Xunzhi Xiang et.al., Paper: [http://arxiv.org/abs/2508.03334](http://arxiv.org/abs/2508.03334)\n", "2508.03254": "|**2025-08-05**|**V.I.P. : Iterative Online Preference Distillation for Efficient Video Diffusion Models**|Jisoo Kim et.al.|[2508.03254](http://arxiv.org/abs/2508.03254)|null|\n", "2508.03034": "|**2025-08-13**|**MoCA: Identity-Preserving Text-to-Video Generation via Mixture of Cross Attention**|Qi Xie et.al.|[2508.03034](http://arxiv.org/abs/2508.03034)|null|\n", "2508.02807": "- 2025-08-04, **DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a Stage-Wise Diffusion Transformer Framework**, Tongchun Zuo et.al., Paper: [http://arxiv.org/abs/2508.02807](http://arxiv.org/abs/2508.02807)\n", "2508.02129": "|**2025-08-04**|**VDEGaussian: Video Diffusion Enhanced 4D Gaussian Splatting for Dynamic Urban Scenes Modeling**|Yuru Xiao et.al.|[2508.02129](http://arxiv.org/abs/2508.02129)|null|\n", "2508.01698": "|**2025-08-03**|**Versatile Transition Generation with Image-to-Video Diffusion**|Zuhao Yang et.al.|[2508.01698](http://arxiv.org/abs/2508.01698)|null|\n", "2508.00795": "- 2025-08-01, **Video Generators are Robot Policies**, Junbang Liang et.al., Paper: [http://arxiv.org/abs/2508.00795](http://arxiv.org/abs/2508.00795)\n", "2508.00397": "- 2025-08-01, **Video Forgery Detection with Optical Flow Residuals and Spatial-Temporal Consistency**, Xi Xue et.al., Paper: [http://arxiv.org/abs/2508.00397](http://arxiv.org/abs/2508.00397)\n", "2508.00312": "- 2025-08-01, **GV-VAD : Exploring Video Generation for Weakly-Supervised Video Anomaly Detection**, Suhang Cai et.al., Paper: [http://arxiv.org/abs/2508.00312](http://arxiv.org/abs/2508.00312)\n", "2508.00289": "|**2025-08-01**|**TITAN-Guide: Taming Inference-Time AligNment for Guided Text-to-Video Diffusion Models**|Christian Simon et.al.|[2508.00289](http://arxiv.org/abs/2508.00289)|null|\n", "2508.00144": "- 2025-07-31, **World Consistency Score: A Unified Metric for Video Generation Quality**, Akshat Rakheja et.al., Paper: [http://arxiv.org/abs/2508.00144](http://arxiv.org/abs/2508.00144)\n", "2507.22360": "|**2025-07-30**|**GVD: Guiding Video Diffusion Model for Scalable Video Distillation**|Kunyang Li et.al.|[2507.22360](http://arxiv.org/abs/2507.22360)|null|\n", "2507.20158": "|**2025-07-27**|**AnimeColor: Reference-based Animation Colorization with Diffusion Transformers**|Yuhong Zhang et.al.|[2507.20158](http://arxiv.org/abs/2507.20158)|**[link](https://github.com/IamCreateAI/AnimeColor)**|\n", "2507.19924": "- 2025-08-01, **HumanSAM: Classifying Human-centric Forgery Videos in Human Spatial, Appearance, and Motion Anomaly**, Chang Liu et.al., Paper: [http://arxiv.org/abs/2507.19924](http://arxiv.org/abs/2507.19924)\n", "2507.19789": "|**2025-07-26**|**TransFlow: Motion Knowledge Transfer from Video Diffusion Models to Video Salient Object Detection**|Suhwan Cho et.al.|[2507.19789](http://arxiv.org/abs/2507.19789)|null|\n", "2507.19138": "|**2025-07-25**|**RealisVSR: Detail-enhanced Diffusion for Real-World 4K Video Super-Resolution**|Weisong Zhao et.al.|[2507.19138](http://arxiv.org/abs/2507.19138)|null|\n", "2507.16341": "|**2025-07-22**|**Navigating Large-Pose Challenge for High-Fidelity Face Reenactment with Video Diffusion Model**|Mingtao Guo et.al.|[2507.16341](http://arxiv.org/abs/2507.16341)|null|\n", "2507.16869": "- 2025-07-22, **Controllable Video Generation: A Survey**, Yue Ma et.al., Paper: [http://arxiv.org/abs/2507.16869](http://arxiv.org/abs/2507.16869)\n", "2507.16116": "|**2025-07-22**|**PUSA V1.0: Surpassing Wan-I2V with $500 Training Cost by Vectorized Timestep Adaptation**|Yaofang Liu et.al.|[2507.16116](http://arxiv.org/abs/2507.16116)|null|\n", "2507.15979": "|**2025-07-21**|**Dream, Lift, Animate: From Single Images to Animatable Gaussian Avatars**|Marcel C. B\u00fchler et.al.|[2507.15979](http://arxiv.org/abs/2507.15979)|null|\n", "2507.15824": "- 2025-07-21, **Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical Commonsense Understanding in Video Generation Models**, Enes Sanli et.al., Paper: [http://arxiv.org/abs/2507.15824](http://arxiv.org/abs/2507.15824)\n", "2507.15728": "|**2025-07-21**|**TokensGen: Harnessing Condensed Tokens for Long Video Generation**|Wenqi Ouyang et.al.|[2507.15728](http://arxiv.org/abs/2507.15728)|**[link](https://github.com/Vicky0522/TokensGen)**|\n", "2507.15260": "|**2025-07-21**|**CHORDS: Diffusion Sampling Accelerator with Multi-core Hierarchical ODE Solvers**|Jiaqi Han et.al.|[2507.15260](http://arxiv.org/abs/2507.15260)|**[link](https://github.com/hanjq17/CHORDS)**|\n", "2507.15064": "|**2025-07-20**|**StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human Image Animation**|Shuyuan Tu et.al.|[2507.15064](http://arxiv.org/abs/2507.15064)|null|\n", "2507.13428": "- 2025-07-17, **\"PhyWorldBench\": A Comprehensive Evaluation of Physical Realism in Text-to-Video Models**, Jing Gu et.al., Paper: [http://arxiv.org/abs/2507.13428](http://arxiv.org/abs/2507.13428)\n", "2507.12898": "|**2025-07-27**|**Vidar: Embodied Video Diffusion Model for Generalist Bimanual Manipulation**|Yao Feng et.al.|[2507.12898](http://arxiv.org/abs/2507.12898)|null|\n", "2507.12646": "|**2025-07-16**|**Reconstruct, Inpaint, Finetune: Dynamic Novel-view Synthesis from Monocular Videos**|Kaihua Chen et.al.|[2507.12646](http://arxiv.org/abs/2507.12646)|null|\n", "2507.11245": "- 2025-07-29, **NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation Models**, X. Feng et.al., Paper: [http://arxiv.org/abs/2507.11245](http://arxiv.org/abs/2507.11245), Code: **[https://github.com/AMAP-ML/NarrLV](https://github.com/AMAP-ML/NarrLV)**\n", "2507.07982": "|**2025-07-10**|**Geometry Forcing: Marrying Video Diffusion and 3D Representation for Consistent World Modeling**|Haoyu Wu et.al.|[2507.07982](http://arxiv.org/abs/2507.07982)|null|\n", "2507.07966": "- 2025-07-30, **Scaling RL to Long Videos**, Yukang Chen et.al., Paper: [http://arxiv.org/abs/2507.07966](http://arxiv.org/abs/2507.07966)\n", "2507.07202": "- 2025-07-09, **A Survey on Long-Video Storytelling Generation: Architectures, Consistency, and Cinematic Quality**, Mohamed Elmoghany et.al., Paper: [http://arxiv.org/abs/2507.07202](http://arxiv.org/abs/2507.07202)\n", "2507.06830": "- 2025-07-09, **Physics-Grounded Motion Forecasting via Equation Discovery for Trajectory-Guided Image-to-Video Generation**, Tao Feng et.al., Paper: [http://arxiv.org/abs/2507.06830](http://arxiv.org/abs/2507.06830)\n", "2507.06812": "- 2025-07-14, **Democratizing High-Fidelity Co-Speech Gesture Video Generation**, Xu Yang et.al., Paper: [http://arxiv.org/abs/2507.06812](http://arxiv.org/abs/2507.06812)\n", "2507.06133": "|**2025-07-24**|**Bridging Sequential Deep Operator Network and Video Diffusion: Residual Refinement of Spatio-Temporal PDE Solutions**|Jaewan Park et.al.|[2507.06133](http://arxiv.org/abs/2507.06133)|null|\n", "2507.05763": "|**2025-07-08**|**DreamArt: Generating Interactable Articulated Objects from a Single Image**|Ruijie Lu et.al.|[2507.05763](http://arxiv.org/abs/2507.05763)|null|\n", "2507.05678": "|**2025-07-08**|**LiON-LoRA: Rethinking LoRA Fusion to Unify Controllable Spatial and Temporal Generation for Video Diffusion**|Yisu Zhang et.al.|[2507.05678](http://arxiv.org/abs/2507.05678)|null|\n", "2507.05675": "- 2025-07-08, **MedGen: Unlocking Medical Video Generation by Scaling Granularly-annotated Medical Videos**, Rongsheng Wang et.al., Paper: [http://arxiv.org/abs/2507.05675](http://arxiv.org/abs/2507.05675), Code: **[https://github.com/FreedomIntelligence/MedGen](https://github.com/FreedomIntelligence/MedGen)**\n", "2507.05198": "|**2025-07-07**|**EmbodieDreamer: Advancing Real2Sim2Real Transfer for Policy Training via Embodied World Modeling**|Boyuan Wang et.al.|[2507.05198](http://arxiv.org/abs/2507.05198)|null|\n", "2507.03745": "- 2025-07-08, **StreamDiT: Real-Time Streaming Text-to-Video Generation**, Akio Kodaira et.al., Paper: [http://arxiv.org/abs/2507.03745](http://arxiv.org/abs/2507.03745)\n", "2507.02862": "- 2025-07-03, **RefTok: Reference-Based Tokenization for Video Generation**, Xiang Fan et.al., Paper: [http://arxiv.org/abs/2507.02862](http://arxiv.org/abs/2507.02862)\n", "2507.02860": "- 2025-07-03, **Less is Enough: Training-Free Video Diffusion Acceleration via Runtime-Adaptive Caching**, Xin Zhou et.al., Paper: [http://arxiv.org/abs/2507.02860](http://arxiv.org/abs/2507.02860)\n", "2507.02813": "|**2025-07-03**|**LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with TriMap Video Diffusion**|Fangfu Liu et.al.|[2507.02813](http://arxiv.org/abs/2507.02813)|**[link](https://github.com/liuff19/LangScene-X)**|\n", "2507.02299": "|**2025-07-03**|**DreamComposer++: Empowering Diffusion Models with Multi-View Conditions for 3D Content Generation**|Yunhan Yang et.al.|[2507.02299](http://arxiv.org/abs/2507.02299)|null|\n", "2507.01945": "- 2025-07-09, **LongAnimation: Long Animation Generation with Dynamic Global-Local Memory**, Nan Chen et.al., Paper: [http://arxiv.org/abs/2507.01945](http://arxiv.org/abs/2507.01945)\n", "2507.01099": "- 2025-07-01, **Geometry-aware 4D Video Generation for Robot Manipulation**, Zeyi Liu et.al., Paper: [http://arxiv.org/abs/2507.01099](http://arxiv.org/abs/2507.01099)\n", "2507.01012": "|**2025-07-01**|**DAM-VSR: Disentanglement of Appearance and Motion for Video Super-Resolution**|Zhe Kong et.al.|[2507.01012](http://arxiv.org/abs/2507.01012)|null|\n", "2507.00990": "|**2025-07-04**|**Robotic Manipulation by Imitating Generated Videos Without Physical Demonstrations**|Shivansh Patel et.al.|[2507.00990](http://arxiv.org/abs/2507.00990)|null|\n", "2507.00334": "- 2025-07-01, **Populate-A-Scene: Affordance-Aware Human Video Generation**, Mengyi Shan et.al., Paper: [http://arxiv.org/abs/2507.00334](http://arxiv.org/abs/2507.00334)\n", "2507.00162": "- 2025-06-30, **FreeLong++: Training-Free Long Video Generation via Multi-band SpectralFusion**, Yu Lu et.al., Paper: [http://arxiv.org/abs/2507.00162](http://arxiv.org/abs/2507.00162)\n", "2506.24113": "|**2025-06-30**|**Epona: Autoregressive Diffusion World Model for Autonomous Driving**|Kaiwen Zhang et.al.|[2506.24113](http://arxiv.org/abs/2506.24113)|**[link](https://github.com/Kevin-thu/Epona)**|\n", "2506.23858": "|**2025-06-30**|**VMoBA: Mixture-of-Block Attention for Video Diffusion Models**|Jianzong Wu et.al.|[2506.23858](http://arxiv.org/abs/2506.23858)|**[link](https://github.com/KwaiVGI/VMoBA)**|\n", "2506.23690": "- 2025-06-30, **SynMotion: Semantic-Visual Adaptation for Motion Customized Video Generation**, Shuai Tan et.al., Paper: [http://arxiv.org/abs/2506.23690](http://arxiv.org/abs/2506.23690)\n", "2506.23263": "|**2025-06-29**|**Causal-Entity Reflected Egocentric Traffic Accident Video Synthesis**|Lei-lei Li et.al.|[2506.23263](http://arxiv.org/abs/2506.23263)|null|\n", "2506.22832": "- 2025-07-01, **Listener-Rewarded Thinking in VLMs for Image Preferences**, Alexander Gambashidze et.al., Paper: [http://arxiv.org/abs/2506.22832](http://arxiv.org/abs/2506.22832)\n", "2506.22432": "|**2025-06-27**|**Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy**|Yuhao Liu et.al.|[2506.22432](http://arxiv.org/abs/2506.22432)|null|\n", "2506.22007": "- 2025-06-27, **RoboEnvision: A Long-Horizon Video Generation Model for Multi-Task Robot Manipulation**, Liudi Yang et.al., Paper: [http://arxiv.org/abs/2506.22007](http://arxiv.org/abs/2506.22007)\n", "2506.21272": "|**2025-06-27**|**FairyGen: Storied Cartoon Video from a Single Child-Drawn Character**|Jiayi Zheng et.al.|[2506.21272](http://arxiv.org/abs/2506.21272)|null|\n", "2506.20946": "- 2025-06-26, **Consistent Zero-shot 3D Texture Synthesis Using Geometry-aware Diffusion and Temporal Video Models**, Donggoo Kang et.al., Paper: [http://arxiv.org/abs/2506.20946](http://arxiv.org/abs/2506.20946)\n", "2506.20756": "|**2025-06-30**|**StereoDiff: Stereo-Diffusion Synergy for Video Depth Estimation**|Haodong Li et.al.|[2506.20756](http://arxiv.org/abs/2506.20756)|null|\n", "2506.20601": "- 2025-06-25, **Video Perception Models for 3D Scene Synthesis**, Rui Huang et.al., Paper: [http://arxiv.org/abs/2506.20601](http://arxiv.org/abs/2506.20601)\n", "2506.20342": "|**2025-06-25**|**Feature Hallucination for Self-supervised Action Recognition**|Lei Wang et.al.|[2506.20342](http://arxiv.org/abs/2506.20342)|null|\n", "2506.19852": "|**2025-06-24**|**Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation**|Xingyang Li et.al.|[2506.19852](http://arxiv.org/abs/2506.19852)|null|\n", "2506.19851": "|**2025-06-24**|**AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion Models**|Zehuan Huang et.al.|[2506.19851](http://arxiv.org/abs/2506.19851)|null|\n", "2506.19840": "|**2025-06-24**|**GenHSI: Controllable Generation of Human-Scene Interaction Videos**|Zekun Li et.al.|[2506.19840](http://arxiv.org/abs/2506.19840)|null|\n", "2506.19798": "|**2025-06-24**|**CoCo4D: Comprehensive and Complex 4D Scene Generation**|Junwei Zhou et.al.|[2506.19798](http://arxiv.org/abs/2506.19798)|null|\n", "2506.19348": "- 2025-06-24, **Training-Free Motion Customization for Distilled Video Generators with Adaptive Test-Time Distillation**, Jintao Rong et.al., Paper: [http://arxiv.org/abs/2506.19348](http://arxiv.org/abs/2506.19348)\n", "2506.18899": "- 2025-06-23, **FilMaster: Bridging Cinematic Principles and Generative AI for Automated Film Generation**, Kaiyi Huang et.al., Paper: [http://arxiv.org/abs/2506.18899](http://arxiv.org/abs/2506.18899)\n", "2506.18897": "- 2025-06-23, **MinD: Unified Visual Imagination and Control via Hierarchical World Models**, Xiaowei Chi et.al., Paper: [http://arxiv.org/abs/2506.18897](http://arxiv.org/abs/2506.18897)\n", "2506.18866": "- 2025-06-23, **OmniAvatar: Efficient Audio-Driven Avatar Video Generation with Adaptive Body Animation**, Qijun Gan et.al., Paper: [http://arxiv.org/abs/2506.18866](http://arxiv.org/abs/2506.18866)\n", "2506.18655": "|**2025-06-23**|**RDPO: Real Data Preference Optimization for Physics Consistency Video Generation**|Wenxu Qian et.al.|[2506.18655](http://arxiv.org/abs/2506.18655)|null|\n", "2506.18601": "- 2025-06-23, **BulletGen: Improving 4D Reconstruction with Bullet-Time Generation**, Denys Rozumnyi et.al., Paper: [http://arxiv.org/abs/2506.18601](http://arxiv.org/abs/2506.18601)\n", "2506.18564": "- 2025-06-23, **VQ-Insight: Teaching VLMs for AI-Generated Video Quality Understanding via Progressive Visual Reinforcement Learning**, Xuanyu Zhang et.al., Paper: [http://arxiv.org/abs/2506.18564](http://arxiv.org/abs/2506.18564)\n", "2502.00360": "- 2025-08-16, **Shape from Semantics: 3D Shape Generation from Multi-View Semantics**, Liangchen Li et.al., Paper: [http://arxiv.org/abs/2502.00360](http://arxiv.org/abs/2502.00360)\n", "2412.07772": "- 2025-08-12, **From Slow Bidirectional to Fast Autoregressive Video Diffusion Models**, Tianwei Yin et.al., Paper: [http://arxiv.org/abs/2412.07772](http://arxiv.org/abs/2412.07772)\n", "2411.13552": "- 2025-08-12, **REDUCIO! Generating 1K Video within 16 Seconds using Extremely Compressed Motion Latents**, Rui Tian et.al., Paper: [http://arxiv.org/abs/2411.13552](http://arxiv.org/abs/2411.13552)\n", "2505.08444": "- 2025-08-07, **Extracting Visual Plans from Unlabeled Videos via Symbolic Guidance**, Wenyan Yang et.al., Paper: [http://arxiv.org/abs/2505.08444](http://arxiv.org/abs/2505.08444)\n", "2501.08545": "- 2025-08-06, **T2VEval: Benchmark Dataset and Objective Evaluation Method for T2V-generated Videos**, Zelu Qi et.al., Paper: [http://arxiv.org/abs/2501.08545](http://arxiv.org/abs/2501.08545)\n", "2410.13287": "- 2025-08-06, **PAK-UCB Contextual Bandit: An Online Learning Approach to Prompt-Aware Selection of Generative Models and LLMs**, Xiaoyan Hu et.al., Paper: [http://arxiv.org/abs/2410.13287](http://arxiv.org/abs/2410.13287)\n", "2505.12620": "- 2025-07-31, **BusterX: MLLM-Powered AI-Generated Video Forgery Detection and Explanation**, Haiquan Wen et.al., Paper: [http://arxiv.org/abs/2505.12620](http://arxiv.org/abs/2505.12620)\n", "2503.09631": "- 2025-07-29, **V2M4: 4D Mesh Animation Reconstruction from a Single Monocular Video**, Jianqi Chen et.al., Paper: [http://arxiv.org/abs/2503.09631](http://arxiv.org/abs/2503.09631)\n", "2502.07327": "- 2025-07-29, **Generative Ghost: Investigating Ranking Bias Hidden in AI-Generated Videos**, Haowen Gao et.al., Paper: [http://arxiv.org/abs/2502.07327](http://arxiv.org/abs/2502.07327), Code: **[https://github.com/Siaaaaaa1/video-source-bias](https://github.com/Siaaaaaa1/video-source-bias)**\n", "2503.18945": "- 2025-07-28, **Aether: Geometric-Aware Unified World Modeling**, Aether Team et.al., Paper: [http://arxiv.org/abs/2503.18945](http://arxiv.org/abs/2503.18945), Code: **[https://github.com/InternRobotics/Aether](https://github.com/InternRobotics/Aether)**\n", "2411.12789": "- 2025-07-26, **Efficient Physics Simulation for 3D Scenes via MLLM-Guided Gaussian Splatting**, Haoyu Zhao et.al., Paper: [http://arxiv.org/abs/2411.12789](http://arxiv.org/abs/2411.12789)\n", "2410.07151": "- 2025-07-13, **DH-FaceVid-1K: A Large-Scale High-Quality Dataset for Face Video Generation**, Donglin Di et.al., Paper: [http://arxiv.org/abs/2410.07151](http://arxiv.org/abs/2410.07151)\n", "2505.07449": "- 2025-07-12, **Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video Generation Model**, Wei Li et.al., Paper: [http://arxiv.org/abs/2505.07449](http://arxiv.org/abs/2505.07449)\n", "2411.04709": "- 2025-07-09, **TIP-I2V: A Million-Scale Real Text and Image Prompt Dataset for Image-to-Video Generation**, Wenhao Wang et.al., Paper: [http://arxiv.org/abs/2411.04709](http://arxiv.org/abs/2411.04709), Code: **[https://github.com/WangWenhao0716/TIP-I2V](https://github.com/WangWenhao0716/TIP-I2V)**\n", "2502.01061": "- 2025-06-30, **OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models**, Gaojie Lin et.al., Paper: [http://arxiv.org/abs/2502.01061](http://arxiv.org/abs/2502.01061)\n", "2506.09113": "- 2025-06-28, **Seedance 1.0: Exploring the Boundaries of Video Generation Models**, Yu Gao et.al., Paper: [http://arxiv.org/abs/2506.09113](http://arxiv.org/abs/2506.09113)\n", "2505.22016": "- 2025-06-25, **PanoWan: Lifting Diffusion Video Generation Models to 360\u00b0 with Latitude/Longitude-aware Mechanisms**, Yifei Xia et.al., Paper: [http://arxiv.org/abs/2505.22016](http://arxiv.org/abs/2505.22016)\n", "2411.14499": "- 2025-06-25, **Understanding World or Predicting Future? A Comprehensive Survey of World Models**, Jingtao Ding et.al., Paper: [http://arxiv.org/abs/2411.14499](http://arxiv.org/abs/2411.14499)\n", "2411.02385": "- 2025-06-22, **How Far is Video Generation from World Model: A Physical Law Perspective**, Bingyi Kang et.al., Paper: [http://arxiv.org/abs/2411.02385](http://arxiv.org/abs/2411.02385)\n", "2412.11224": "- 2025-06-20, **GenLit: Reformulating Single-Image Relighting as Video Generation**, Shrisha Bharadwaj et.al., Paper: [http://arxiv.org/abs/2412.11224](http://arxiv.org/abs/2412.11224)\n", "2506.16209": "- 2025-06-19, **VideoGAN-based Trajectory Proposal for Automated Vehicles**, Annajoyce Mariani et.al., Paper: [http://arxiv.org/abs/2506.16209](http://arxiv.org/abs/2506.16209), Code: **[https://github.com/ajmariani/video-gan-trajectories](https://github.com/ajmariani/video-gan-trajectories)**\n", "2506.15980": "- 2025-06-19, **Advanced Sign Language Video Generation with Compressed and Quantized Multi-Condition Tokenization**, Cong Wang et.al., Paper: [http://arxiv.org/abs/2506.15980](http://arxiv.org/abs/2506.15980)\n", "2505.12705": "- 2025-06-17, **DreamGen: Unlocking Generalization in Robot Learning through Video World Models**, Joel Jang et.al., Paper: [http://arxiv.org/abs/2505.12705](http://arxiv.org/abs/2505.12705)\n", "2506.13691": "- 2025-06-16, **UltraVideo: High-Quality UHD Video Dataset with Comprehensive Captions**, Zhucun Xue et.al., Paper: [http://arxiv.org/abs/2506.13691](http://arxiv.org/abs/2506.13691)\n", "2501.12375": "- 2025-06-15, **Video Depth Anything: Consistent Depth Estimation for Super-Long Videos**, Sili Chen et.al., Paper: [http://arxiv.org/abs/2501.12375](http://arxiv.org/abs/2501.12375)\n", "2505.03205": "- 2025-06-13, **Transformers for Learning on Noisy and Task-Level Manifolds: Approximation and Generalization Insights**, Zhaiming Shen et.al., Paper: [http://arxiv.org/abs/2505.03205](http://arxiv.org/abs/2505.03205)\n", "2506.04830": "- 2025-06-13, **DualX-VSR: Dual Axial Spatial$\\times$Temporal Transformer for Real-World Video Super-Resolution without Motion Compensation**, Shuo Cao et.al., Paper: [http://arxiv.org/abs/2506.04830](http://arxiv.org/abs/2506.04830)\n", "2506.10975": "- 2025-06-12, **GenWorld: Towards Detecting AI-generated Real-world Simulation Videos**, Weiliang Chen et.al., Paper: [http://arxiv.org/abs/2506.10975](http://arxiv.org/abs/2506.10975)\n", "2506.10540": "- 2025-06-12, **AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven Clip Generation**, Haoyuan Shi et.al., Paper: [http://arxiv.org/abs/2506.10540](http://arxiv.org/abs/2506.10540)\n", "2506.05343": "- 2025-06-11, **ContentV: Efficient Training of Video Generation Models with Limited Compute**, Wenfeng Lin et.al., Paper: [http://arxiv.org/abs/2506.05343](http://arxiv.org/abs/2506.05343)\n", "2506.09644": "- 2025-06-11, **DGAE: Diffusion-Guided Autoencoder for Efficient Latent Representation Learning**, Dongxu Liu et.al., Paper: [http://arxiv.org/abs/2506.09644](http://arxiv.org/abs/2506.09644)\n", "2506.09350": "- 2025-06-11, **Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation**, Shanchuan Lin et.al., Paper: [http://arxiv.org/abs/2506.09350](http://arxiv.org/abs/2506.09350)\n", "2410.15461": "- 2025-06-10, **EVA: An Embodied World Model for Future Video Anticipation**, Xiaowei Chi et.al., Paper: [http://arxiv.org/abs/2410.15461](http://arxiv.org/abs/2410.15461)\n", "2505.22944": "- 2025-06-10, **ATI: Any Trajectory Instruction for Controllable Video Generation**, Angtian Wang et.al., Paper: [http://arxiv.org/abs/2505.22944](http://arxiv.org/abs/2505.22944)\n", "2506.08351": "- 2025-06-10, **How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models**, Huixuan Zhang et.al., Paper: [http://arxiv.org/abs/2506.08351](http://arxiv.org/abs/2506.08351)\n", "2506.08279": "- 2025-06-09, **Seeing Voices: Generating A-Roll Video from Audio with Mirage**, Aditi Sundararaman et.al., Paper: [http://arxiv.org/abs/2506.08279](http://arxiv.org/abs/2506.08279)\n", "2412.10494": "- 2025-06-09, **SnapGen-V: Generating a Five-Second Video within Five Seconds on a Mobile Device**, Yushu Wu et.al., Paper: [http://arxiv.org/abs/2412.10494](http://arxiv.org/abs/2412.10494)\n", "2506.08006": "- 2025-06-09, **Dreamland: Controllable World Creation with Simulator and Generative Models**, Sicheng Mo et.al., Paper: [http://arxiv.org/abs/2506.08006](http://arxiv.org/abs/2506.08006)\n", "2506.07891": "- 2025-06-09, **Video Unlearning via Low-Rank Refusal Vector**, Simone Facchiano et.al., Paper: [http://arxiv.org/abs/2506.07891](http://arxiv.org/abs/2506.07891)\n", "2505.21036": "- 2025-06-09, **RainFusion: Adaptive Video Generation Acceleration via Multi-Dimensional Visual Redundancy**, Aiyue Chen et.al., Paper: [http://arxiv.org/abs/2505.21036](http://arxiv.org/abs/2505.21036)\n", "2506.07205": "- 2025-06-08, **TV-LiVE: Training-Free, Text-Guided Video Editing via Layer Informed Vitality Exploitation**, Min-Jung Kim et.al., Paper: [http://arxiv.org/abs/2506.07205](http://arxiv.org/abs/2506.07205)\n", "2501.06173": "- 2025-06-07, **VideoAuteur: Towards Long Narrative Video Generation**, Junfei Xiao et.al., Paper: [http://arxiv.org/abs/2501.06173](http://arxiv.org/abs/2501.06173)\n", "2506.06658": "- 2025-06-07, **Self-Adapting Improvement Loops for Robotic Learning**, Calvin Luo et.al., Paper: [http://arxiv.org/abs/2506.06658](http://arxiv.org/abs/2506.06658)\n", "2506.03099": "- 2025-06-03, **TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via Autoregressive Diffusion Models**, Chetwin Low et.al., Paper: [http://arxiv.org/abs/2506.03099](http://arxiv.org/abs/2506.03099)\n", "2505.19901": "- 2025-06-03, **Dynamic-I2V: Exploring Image-to-Video Generation Models via Multimodal LLM**, Peng Liu et.al., Paper: [http://arxiv.org/abs/2505.19901](http://arxiv.org/abs/2505.19901)\n", "2506.02244": "- 2025-06-02, **Motion aware video generative model**, Bowen Xue et.al., Paper: [http://arxiv.org/abs/2506.02244](http://arxiv.org/abs/2506.02244), Code: **[https://github.com/USTCPCS/CVPR2018_attention](https://github.com/USTCPCS/CVPR2018_attention)**\n", "2506.01466": "- 2025-06-02, **Towards Scalable Video Anomaly Retrieval: A Synthetic Video-Text Benchmark**, Shuyu Yang et.al., Paper: [http://arxiv.org/abs/2506.01466](http://arxiv.org/abs/2506.01466), Code: **[https://github.com/Shuyu-XJTU/SVTA](https://github.com/Shuyu-XJTU/SVTA)**\n", "2412.05718": "- 2025-06-01, **RLZero: Direct Policy Inference from Language Without In-Domain Supervision**, Harshit Sikchi et.al., Paper: [http://arxiv.org/abs/2412.05718](http://arxiv.org/abs/2412.05718)\n", "2506.00613": "- 2025-05-31, **Evaluating Robot Policies in a World Model**, Julian Quevedo et.al., Paper: [http://arxiv.org/abs/2506.00613](http://arxiv.org/abs/2506.00613), Code: **[https://github.com/EnnaSachdeva/Recurrent-Multiagent-Deep-Deterministic-Policy-Gradient-with-Difference-Rewards](https://github.com/EnnaSachdeva/Recurrent-Multiagent-Deep-Deterministic-Policy-Gradient-with-Difference-Rewards)**\n", "2506.00227": "- 2025-05-30, **Ctrl-Crash: Controllable Diffusion for Realistic Car Crashes**, Anthony Gosselin et.al., Paper: [http://arxiv.org/abs/2506.00227](http://arxiv.org/abs/2506.00227)\n", "2505.24873": "- 2025-05-30, **MiniMax-Remover: Taming Bad Noise Helps Video Object Removal**, Bojia Zi et.al., Paper: [http://arxiv.org/abs/2505.24873](http://arxiv.org/abs/2505.24873), Code: **[https://github.com/zibojia/MiniMax-Remover](https://github.com/zibojia/MiniMax-Remover)**\n", "2505.24733": "- 2025-05-30, **DreamDance: Animating Character Art via Inpainting Stable Gaussian Worlds**, Jiaxu Zhang et.al., Paper: [http://arxiv.org/abs/2505.24733](http://arxiv.org/abs/2505.24733)\n", "2505.24521": "- 2025-05-30, **UniGeo: Taming Video Diffusion for Unified Consistent Geometry Estimation**, Yang-Tian Sun et.al., Paper: [http://arxiv.org/abs/2505.24521](http://arxiv.org/abs/2505.24521)\n", "2503.17359": "- 2025-05-29, **Position: Interactive Generative Video as Next-Generation Game Engine**, Jiwen Yu et.al., Paper: [http://arxiv.org/abs/2503.17359](http://arxiv.org/abs/2503.17359)\n", "2409.04003": "- 2025-05-29, **DreamForge: Motion-Aware Autoregressive Video Generation for Multi-View Driving Scenes**, Jianbiao Mei et.al., Paper: [http://arxiv.org/abs/2409.04003](http://arxiv.org/abs/2409.04003)\n", "2505.23325": "- 2025-05-29, **Dimension-Reduction Attack! Video Generative Models are Experts on Controllable Image Synthesis**, Hengyuan Cao et.al., Paper: [http://arxiv.org/abs/2505.23325](http://arxiv.org/abs/2505.23325)\n", "2505.23134": "- 2025-05-29, **Zero-to-Hero: Zero-Shot Initialization Empowering Reference-Based Video Appearance Editing**, Tongtong Su et.al., Paper: [http://arxiv.org/abs/2505.23134](http://arxiv.org/abs/2505.23134)\n", "2505.23120": "- 2025-05-29, **MMGT: Motion Mask Guided Two-Stage Network for Co-Speech Gesture Video Generation**, Siyuan Wang et.al., Paper: [http://arxiv.org/abs/2505.23120](http://arxiv.org/abs/2505.23120)\n", "2503.19385": "- 2025-05-28, **Inference-Time Scaling for Flow Models via Stochastic Generation and Rollover Budget Forcing**, Jaihoon Kim et.al., Paper: [http://arxiv.org/abs/2503.19385](http://arxiv.org/abs/2503.19385)\n", "2505.14135": "- 2025-05-28, **Hunyuan-Game: Industrial-grade Intelligent Game Creation Model**, Ruihuang Li et.al., Paper: [http://arxiv.org/abs/2505.14135](http://arxiv.org/abs/2505.14135)\n", "2505.21996": "- 2025-05-28, **Learning World Models for Interactive Video Generation**, Taiye Chen et.al., Paper: [http://arxiv.org/abs/2505.21996](http://arxiv.org/abs/2505.21996)\n", "2505.21620": "- 2025-05-27, **VideoMarkBench: Benchmarking Robustness of Video Watermarking**, Zhengyuan Jiang et.al., Paper: [http://arxiv.org/abs/2505.21620](http://arxiv.org/abs/2505.21620)\n", "2505.20795": "- 2025-05-27, **Learning Generalizable Robot Policy with Human Demonstration Video as a Prompt**, Xiang Zhu et.al., Paper: [http://arxiv.org/abs/2505.20795](http://arxiv.org/abs/2505.20795)\n", "2505.19386": "- 2025-05-26, **Force Prompting: Video Generation Models Can Learn and Generalize Physics-based Control Signals**, Nate Gillman et.al., Paper: [http://arxiv.org/abs/2505.19386](http://arxiv.org/abs/2505.19386)\n", "2505.19306": "- 2025-05-25, **From Single Images to Motion Policies via Video-Generation Environment Representations**, Weiming Zhi et.al., Paper: [http://arxiv.org/abs/2505.19306](http://arxiv.org/abs/2505.19306)\n", "2505.15800": "- 2025-05-25, **Interspatial Attention for Efficient 4D Human Video Generation**, Ruizhi Shao et.al., Paper: [http://arxiv.org/abs/2505.15800](http://arxiv.org/abs/2505.15800)\n", "2505.19017": "- 2025-05-25, **WorldEval: World Model as Real-World Robot Policies Evaluator**, Yaxuan Li et.al., Paper: [http://arxiv.org/abs/2505.19017](http://arxiv.org/abs/2505.19017)\n", "2505.11497": "- 2025-05-23, **QVGen: Pushing the Limit of Quantized Video Generative Models**, Yushi Huang et.al., Paper: [http://arxiv.org/abs/2505.11497](http://arxiv.org/abs/2505.11497)\n", "2505.17618": "- 2025-05-23, **Scaling Image and Video Generation via Test-Time Evolutionary Search**, Haoran He et.al., Paper: [http://arxiv.org/abs/2505.17618](http://arxiv.org/abs/2505.17618), Code: **[https://github.com/tinnerhrhe/EvoSearch-codes](https://github.com/tinnerhrhe/EvoSearch-codes)**\n", "2505.16456": "- 2025-05-22, **MAGIC: Motion-Aware Generative Inference via Confidence-Guided LLM**, Siwei Meng et.al., Paper: [http://arxiv.org/abs/2505.16456](http://arxiv.org/abs/2505.16456)\n", "2412.10255": "- 2025-05-22, **AniSora: Exploring the Frontiers of Animation Video Generation in the Sora Era**, Yudong Jiang et.al., Paper: [http://arxiv.org/abs/2412.10255](http://arxiv.org/abs/2412.10255)\n", "2505.15145": "- 2025-05-21, **CineTechBench: A Benchmark for Cinematographic Technique Understanding and Generation**, Xinran Wang et.al., Paper: [http://arxiv.org/abs/2505.15145](http://arxiv.org/abs/2505.15145)\n", "2505.09694": "- 2025-05-18, **EWMBench: Evaluating Scene, Motion, and Semantic Quality in Embodied World Models**, Hu Yue et.al., Paper: [http://arxiv.org/abs/2505.09694](http://arxiv.org/abs/2505.09694)\n", "2505.14708": "- 2025-05-17, **DraftAttention: Fast Video Diffusion via Low-Resolution Attention Guidance**, Xuan Shen et.al., Paper: [http://arxiv.org/abs/2505.14708](http://arxiv.org/abs/2505.14708)\n", "2505.10584": "- 2025-05-14, **Aquarius: A Family of Industry-Level Video Generation Models for Marketing Scenarios**, Huafeng Shi et.al., Paper: [http://arxiv.org/abs/2505.10584](http://arxiv.org/abs/2505.10584)\n", "2503.01739": "- 2025-05-13, **VideoUFO: A Million-Scale User-Focused Dataset for Text-to-Video Generation**, Wenhao Wang et.al., Paper: [http://arxiv.org/abs/2503.01739](http://arxiv.org/abs/2503.01739)\n", "2503.03355": "- 2025-05-08, **Rethinking Video Super-Resolution: Towards Diffusion-Based Methods without Motion Alignment**, Zhihao Zhan et.al., Paper: [http://arxiv.org/abs/2503.03355](http://arxiv.org/abs/2503.03355)\n", "2505.04946": "- 2025-05-08, **T2VTextBench: A Human Evaluation Benchmark for Textual Control in Video Generation Models**, Xuyang Guo et.al., Paper: [http://arxiv.org/abs/2505.04946](http://arxiv.org/abs/2505.04946)\n", "2408.10453": "- 2025-05-05, **Kubrick: Multimodal Agent Collaborations for Synthetic Video Generation**, Liu He et.al., Paper: [http://arxiv.org/abs/2408.10453](http://arxiv.org/abs/2408.10453)\n", "2504.08685": "- 2025-05-05, **Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model**, Team Seawead et.al., Paper: [http://arxiv.org/abs/2504.08685](http://arxiv.org/abs/2504.08685)\n", "2410.10821": "- 2025-05-03, **Tex4D: Zero-shot 4D Scene Texturing with Video Diffusion Models**, Jingzhi Bao et.al., Paper: [http://arxiv.org/abs/2410.10821](http://arxiv.org/abs/2410.10821)\n", "2505.00337": "- 2025-05-01, **T2VPhysBench: A First-Principles Benchmark for Physical Consistency in Text-to-Video Generation**, Xuyang Guo et.al., Paper: [http://arxiv.org/abs/2505.00337](http://arxiv.org/abs/2505.00337)\n", "2504.21855": "- 2025-04-30, **ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D Physics Modeling for Complex Motion and Interaction**, Qihao Liu et.al., Paper: [http://arxiv.org/abs/2504.21855](http://arxiv.org/abs/2504.21855)\n", "2504.21334": "- 2025-04-30, **Simple Visual Artifact Detection in Sora-Generated Videos**, Misora Sugiyama et.al., Paper: [http://arxiv.org/abs/2504.21334](http://arxiv.org/abs/2504.21334)\n", "2504.20995": "- 2025-04-29, **TesserAct: Learning 4D Embodied World Models**, Haoyu Zhen et.al., Paper: [http://arxiv.org/abs/2504.20995](http://arxiv.org/abs/2504.20995)\n", "2410.08260": "- 2025-04-26, **Koala-36M: A Large-scale Video Dataset Improving Consistency between Fine-grained Conditions and Video Content**, Qiuheng Wang et.al., Paper: [http://arxiv.org/abs/2410.08260](http://arxiv.org/abs/2410.08260)\n", "2411.16718": "- 2025-04-25, **Neuro-Symbolic Evaluation of Text-to-Video Models using Formal Verification**, S P Sharan et.al., Paper: [http://arxiv.org/abs/2411.16718](http://arxiv.org/abs/2411.16718)\n", "2504.17816": "- 2025-04-23, **Subject-driven Video Generation via Disentangled Identity and Motion**, Daneul Kim et.al., Paper: [http://arxiv.org/abs/2504.17816](http://arxiv.org/abs/2504.17816), Code: **[https://github.com/carpedkm/disentangled-subject-to-vid](https://github.com/carpedkm/disentangled-subject-to-vid)**\n", "2504.15369": "- 2025-04-21, **Solving New Tasks by Adapting Internet Video Knowledge**, Calvin Luo et.al., Paper: [http://arxiv.org/abs/2504.15369](http://arxiv.org/abs/2504.15369), Code: **[https://github.com/brown-palm/adapt2act](https://github.com/brown-palm/adapt2act)**\n", "2504.15182": "- 2025-04-21, **Tiger200K: Manually Curated High Visual Quality Video Dataset from UGC Platform**, Xianpan Zhou et.al., Paper: [http://arxiv.org/abs/2504.15182](http://arxiv.org/abs/2504.15182)\n", "2503.06163": "- 2025-04-20, **VACT: A Video Automatic Causal Testing System and a Benchmark**, Haotong Yang et.al., Paper: [http://arxiv.org/abs/2503.06163](http://arxiv.org/abs/2503.06163)\n", "2503.20314": "- 2025-04-19, **Wan: Open and Advanced Large-Scale Video Generative Models**, Team Wan et.al., Paper: [http://arxiv.org/abs/2503.20314](http://arxiv.org/abs/2503.20314), Code: **[https://github.com/Wan-Video/Wan2.1](https://github.com/Wan-Video/Wan2.1)**\n", "2412.03572": "- 2025-04-11, **Navigation World Models**, Amir Bar et.al., Paper: [http://arxiv.org/abs/2412.03572](http://arxiv.org/abs/2412.03572), Code: **[https://github.com/InternRobotics/Aether](https://github.com/InternRobotics/Aether)**\n", "2504.04153": "- 2025-04-05, **Video4DGen: Enhancing Video and 4D Generation through Mutual Optimization**, Yikai Wang et.al., Paper: [http://arxiv.org/abs/2504.04153](http://arxiv.org/abs/2504.04153), Code: **[https://github.com/yikaiw/Vidu4D](https://github.com/yikaiw/Vidu4D)**\n", "2412.11785": "- 2025-04-04, **InterDyn: Controllable Interactive Dynamics with Video Diffusion Models**, Rick Akkerman et.al., Paper: [http://arxiv.org/abs/2412.11785](http://arxiv.org/abs/2412.11785)\n", "2504.02764": "- 2025-04-03, **Scene Splatter: Momentum 3D Scene Generation from Single Image with Video Diffusion Model**, Shengjun Zhang et.al., Paper: [http://arxiv.org/abs/2504.02764](http://arxiv.org/abs/2504.02764)\n", "2504.02918": "- 2025-04-03, **Morpheus: Benchmarking Physical Reasoning of Video Generative Models with Real Physical Experiments**, Chenyu Zhang et.al., Paper: [http://arxiv.org/abs/2504.02918](http://arxiv.org/abs/2504.02918)\n", "2504.01956": "- 2025-04-03, **VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in One Step**, Hanyang Wang et.al., Paper: [http://arxiv.org/abs/2504.01956](http://arxiv.org/abs/2504.01956)\n", "2504.02045": "- 2025-04-02, **WorldPrompter: Traversable Text-to-Scene Generation**, Zhaoyang Zhang et.al., Paper: [http://arxiv.org/abs/2504.02045](http://arxiv.org/abs/2504.02045)\n", "2502.11897": "- 2025-04-02, **DLFR-VAE: Dynamic Latent Frame Rate VAE for Video Generation**, Zhihang Yuan et.al., Paper: [http://arxiv.org/abs/2502.11897](http://arxiv.org/abs/2502.11897)\n", "2504.00983": "- 2025-04-01, **WorldScore: A Unified Evaluation Benchmark for World Generation**, Haoyi Duan et.al., Paper: [http://arxiv.org/abs/2504.00983](http://arxiv.org/abs/2504.00983)\n", "2503.18942": "- 2025-04-01, **Video-T1: Test-Time Scaling for Video Generation**, Fangfu Liu et.al., Paper: [http://arxiv.org/abs/2503.18942](http://arxiv.org/abs/2503.18942)\n", "2503.23796": "- 2025-04-01, **On-device Sora: Enabling Training-Free Diffusion-based Text-to-Video Generation for Mobile Devices**, Bosung Kim et.al., Paper: [http://arxiv.org/abs/2503.23796](http://arxiv.org/abs/2503.23796)\n", "2503.24379": "- 2025-03-31, **Any2Caption:Interpreting Any Condition to Caption for Controllable Video Generation**, Shengqiong Wu et.al., Paper: [http://arxiv.org/abs/2503.24379](http://arxiv.org/abs/2503.24379)\n", "2502.04363": "- 2025-03-31, **On-device Sora: Enabling Training-Free Diffusion-based Text-to-Video Generation for Mobile Devices**, Bosung Kim et.al., Paper: [http://arxiv.org/abs/2502.04363](http://arxiv.org/abs/2502.04363)\n", "2411.15262": "- 2025-03-31, **MovieBench: A Hierarchical Movie Level Dataset for Long Video Generation**, Weijia Wu et.al., Paper: [http://arxiv.org/abs/2411.15262](http://arxiv.org/abs/2411.15262)\n", "2503.23284": "- 2025-03-30, **SketchVideo: Sketch-based Video Generation and Editing**, Feng-Lin Liu et.al., Paper: [http://arxiv.org/abs/2503.23284](http://arxiv.org/abs/2503.23284)\n", "2412.04471": "- 2025-03-29, **PaintScene4D: Consistent 4D Scene Generation from Text Prompts**, Vinayak Gupta et.al., Paper: [http://arxiv.org/abs/2412.04471](http://arxiv.org/abs/2412.04471)\n", "2412.02700": "- 2025-03-27, **Motion Prompting: Controlling Video Generation with Motion Trajectories**, Daniel Geng et.al., Paper: [http://arxiv.org/abs/2412.02700](http://arxiv.org/abs/2412.02700)\n", "2503.21755": "- 2025-03-27, **VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic Faithfulness**, Dian Zheng et.al., Paper: [http://arxiv.org/abs/2503.21755](http://arxiv.org/abs/2503.21755)\n", "2503.03708": "- 2025-03-27, **Rethinking Video Tokenization: A Conditioned Diffusion-based Approach**, Nianzu Yang et.al., Paper: [http://arxiv.org/abs/2503.03708](http://arxiv.org/abs/2503.03708)\n", "2503.20491": "- 2025-03-26, **VPO: Aligning Text-to-Video Generation Models with Prompt Optimization**, Jiale Cheng et.al., Paper: [http://arxiv.org/abs/2503.20491](http://arxiv.org/abs/2503.20491)\n", "2412.18597": "- 2025-03-26, **DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation**, Minghong Cai et.al., Paper: [http://arxiv.org/abs/2412.18597](http://arxiv.org/abs/2412.18597)\n", "2503.20822": "- 2025-03-26, **Synthetic Video Enhances Physical Fidelity in Video Synthesis**, Qi Zhao et.al., Paper: [http://arxiv.org/abs/2503.20822](http://arxiv.org/abs/2503.20822)\n", "2503.20118": "- 2025-03-25, **Zero-Shot Human-Object Interaction Synthesis with Multimodal Priors**, Yuke Lou et.al., Paper: [http://arxiv.org/abs/2503.20118](http://arxiv.org/abs/2503.20118)\n", "2411.17440": "- 2025-03-25, **Identity-Preserving Text-to-Video Generation by Frequency Decomposition**, Shenghai Yuan et.al., Paper: [http://arxiv.org/abs/2411.17440](http://arxiv.org/abs/2411.17440)\n", "2503.19383": "- 2025-03-25, **MVPortrait: Text-Guided Motion and Emotion Control for Multi-view Vivid Portrait Animation**, Yukang Lin et.al., Paper: [http://arxiv.org/abs/2503.19383](http://arxiv.org/abs/2503.19383)\n", "2503.18386": "- 2025-03-24, **Resource-Efficient Motion Control for Video Generation via Dynamic Mask Guidance**, Sicong Feng et.al., Paper: [http://arxiv.org/abs/2503.18386](http://arxiv.org/abs/2503.18386)\n", "2503.09642": "- 2025-03-23, **Open-Sora 2.0: Training a Commercial-Level Video Generation Model in $200k**, Xiangyu Peng et.al., Paper: [http://arxiv.org/abs/2503.09642](http://arxiv.org/abs/2503.09642)\n", "2503.17934": "- 2025-03-23, **TransAnimate: Taming Layer Diffusion to Generate RGBA Video**, Xuewei Chen et.al., Paper: [http://arxiv.org/abs/2503.17934](http://arxiv.org/abs/2503.17934)\n", "2503.17735": "- 2025-03-22, **RDTF: Resource-efficient Dual-mask Training Framework for Multi-frame Animated Sticker Generation**, Zhiqiang Yuan et.al., Paper: [http://arxiv.org/abs/2503.17735](http://arxiv.org/abs/2503.17735)\n", "2412.18600": "- 2025-03-21, **ZeroHSI: Zero-Shot 4D Human-Scene Interaction by Video Generation**, Hongjie Li et.al., Paper: [http://arxiv.org/abs/2412.18600](http://arxiv.org/abs/2412.18600)\n", "2503.09949": "- 2025-03-21, **UVE: Are MLLMs Unified Evaluators for AI-Generated Videos?**, Yuanxin Liu et.al., Paper: [http://arxiv.org/abs/2503.09949](http://arxiv.org/abs/2503.09949)\n", "2503.17029": "- 2025-03-21, **AnimatePainter: A Self-Supervised Rendering Framework for Reconstructing Painting Process**, Junjie Hu et.al., Paper: [http://arxiv.org/abs/2503.17029](http://arxiv.org/abs/2503.17029)\n", "2503.15138": "- 2025-03-20, **VideoGen-of-Thought: Step-by-step generating multi-shot video with minimal manual intervention**, Mingzhe Zheng et.al., Paper: [http://arxiv.org/abs/2503.15138](http://arxiv.org/abs/2503.15138)\n", "2503.15855": "- 2025-03-20, **VideoRFSplat: Direct Scene-Level Text-to-3D Gaussian Splatting Generation with Flexible Pose and Multi-View Joint Modeling**, Hyojun Go et.al., Paper: [http://arxiv.org/abs/2503.15855](http://arxiv.org/abs/2503.15855)\n", "2412.02259": "- 2025-03-20, **VideoGen-of-Thought: Step-by-step generating multi-shot video with minimal manual intervention**, Mingzhe Zheng et.al., Paper: [http://arxiv.org/abs/2412.02259](http://arxiv.org/abs/2412.02259)\n", "2503.15417": "- 2025-03-19, **Temporal Regularization Makes Your Video Generator Stronger**, Harold Haodong Chen et.al., Paper: [http://arxiv.org/abs/2503.15417](http://arxiv.org/abs/2503.15417)\n", "2503.14378": "- 2025-03-18, **Impossible Videos**, Zechen Bai et.al., Paper: [http://arxiv.org/abs/2503.14378](http://arxiv.org/abs/2503.14378)\n", "2503.14070": "- 2025-03-18, **Fast Autoregressive Video Generation with Diagonal Decoding**, Yang Ye et.al., Paper: [http://arxiv.org/abs/2503.14070](http://arxiv.org/abs/2503.14070)\n", "2503.14064": "- 2025-03-18, **AIGVE-Tool: AI-Generated Video Evaluation Toolkit with Multifaceted Benchmark**, Xinhao Xiang et.al., Paper: [http://arxiv.org/abs/2503.14064](http://arxiv.org/abs/2503.14064)\n", "2506.12103": "- 2025-03-17, **The Amazon Nova Family of Models: Technical Report and Model Card**, Amazon AGI et.al., Paper: [http://arxiv.org/abs/2506.12103](http://arxiv.org/abs/2506.12103)\n", "2503.12535": "- 2025-03-16, **SPC-GS: Gaussian Splatting with Semantic-Prompt Consistency for Indoor Open-World Free-view Synthesis from Sparse Inputs**, Guibiao Liao et.al., Paper: [http://arxiv.org/abs/2503.12535](http://arxiv.org/abs/2503.12535)\n", "2410.05954": "- 2025-03-15, **Pyramidal Flow Matching for Efficient Video Generative Modeling**, Yang Jin et.al., Paper: [http://arxiv.org/abs/2410.05954](http://arxiv.org/abs/2410.05954)\n", "2503.09160": "- 2025-03-15, **WonderVerse: Extendable 3D Scene Generation with Video Generative Models**, Hao Feng et.al., Paper: [http://arxiv.org/abs/2503.09160](http://arxiv.org/abs/2503.09160)\n", "2503.11251": "- 2025-03-14, **Step-Video-TI2V Technical Report: A State-of-the-Art Text-Driven Image-to-Video Generation Model**, Haoyang Huang et.al., Paper: [http://arxiv.org/abs/2503.11251](http://arxiv.org/abs/2503.11251)\n", "2503.10592": "- 2025-03-13, **CameraCtrl II: Dynamic Scene Exploration via Camera-controlled Video Diffusion Models**, Hao He et.al., Paper: [http://arxiv.org/abs/2503.10592](http://arxiv.org/abs/2503.10592)\n", "2412.00733": "- 2025-03-13, **Hallo3: Highly Dynamic and Realistic Portrait Image Animation with Video Diffusion Transformer**, Jiahao Cui et.al., Paper: [http://arxiv.org/abs/2412.00733](http://arxiv.org/abs/2412.00733), Code: **[https://github.com/fudan-generative-vision/hallo3](https://github.com/fudan-generative-vision/hallo3)**\n", "2503.09871": "- 2025-03-12, **LuciBot: Automated Robot Policy Learning from Generated Videos**, Xiaowen Qiu et.al., Paper: [http://arxiv.org/abs/2503.09871](http://arxiv.org/abs/2503.09871)\n", "2503.09595": "- 2025-03-12, **PISA Experiments: Exploring Physics Post-Training for Video Diffusion Models by Watching Stuff Drop**, Chenyu Li et.al., Paper: [http://arxiv.org/abs/2503.09595](http://arxiv.org/abs/2503.09595), Code: **[https://github.com/vision-x-nyu/pisa-experiments](https://github.com/vision-x-nyu/pisa-experiments)**\n", "2503.09675": "- 2025-03-12, **Accelerating Diffusion Sampling via Exploiting Local Transition Coherence**, Shangwen Zhu et.al., Paper: [http://arxiv.org/abs/2503.09675](http://arxiv.org/abs/2503.09675)\n", "2412.03603": "- 2025-03-11, **HunyuanVideo: A Systematic Framework For Large Video Generative Models**, Weijie Kong et.al., Paper: [http://arxiv.org/abs/2412.03603](http://arxiv.org/abs/2412.03603)\n", "2503.08037": "- 2025-03-11, **ObjectMover: Generative Object Movement with Video Prior**, Xin Yu et.al., Paper: [http://arxiv.org/abs/2503.08037](http://arxiv.org/abs/2503.08037)\n", "2503.00276": "- 2025-03-10, **Learning to Animate Images from A Few Videos to Portray Delicate Human Actions**, Haoxin Li et.al., Paper: [http://arxiv.org/abs/2503.00276](http://arxiv.org/abs/2503.00276)\n", "2503.06800": "- 2025-03-09, **VideoPhy-2: A Challenging Action-Centric Physical Commonsense Evaluation in Video Generation**, Hritik Bansal et.al., Paper: [http://arxiv.org/abs/2503.06800](http://arxiv.org/abs/2503.06800)\n", "2501.03699": "- 2025-03-08, **Motion-Aware Generative Frame Interpolation**, Guozhen Zhang et.al., Paper: [http://arxiv.org/abs/2501.03699](http://arxiv.org/abs/2501.03699)\n", "2503.04666": "- 2025-03-06, **What Are You Doing? A Closer Look at Controllable Human Video Generation**, Emanuele Bugliarello et.al., Paper: [http://arxiv.org/abs/2503.04666](http://arxiv.org/abs/2503.04666)\n", "2501.09781": "- 2025-03-05, **VideoWorld: Exploring Knowledge Learning from Unlabeled Videos**, Zhongwei Ren et.al., Paper: [http://arxiv.org/abs/2501.09781](http://arxiv.org/abs/2501.09781)\n", "2502.05503": "- 2025-03-05, **A Physical Coherence Benchmark for Evaluating Video Generation Models via Optical Flow-guided Frame Prediction**, Yongfan Chen et.al., Paper: [http://arxiv.org/abs/2502.05503](http://arxiv.org/abs/2502.05503)\n", "2503.02341": "- 2025-03-04, **GRADEO: Towards Human-Like Evaluation for Text-to-Video Generation via Multi-Step Reasoning**, Zhun Mou et.al., Paper: [http://arxiv.org/abs/2503.02341](http://arxiv.org/abs/2503.02341)\n", "2410.05591": "- 2025-03-04, **TweedieMix: Improving Multi-Concept Fusion for Diffusion-based Image/Video Generation**, Gihyun Kwon et.al., Paper: [http://arxiv.org/abs/2410.05591](http://arxiv.org/abs/2410.05591)\n", "2503.00948": "- 2025-03-02, **Extrapolating and Decoupling Image-to-Video Generation Models: Motion Modeling is Easier Than You Think**, Jie Tian et.al., Paper: [http://arxiv.org/abs/2503.00948](http://arxiv.org/abs/2503.00948)\n", "2502.20694": "- 2025-02-28, **WorldModelBench: Judging Video Generation Models As World Models**, Dacheng Li et.al., Paper: [http://arxiv.org/abs/2502.20694](http://arxiv.org/abs/2502.20694)\n", "2502.07508": "- 2025-02-27, **Enhance-A-Video: Better Generated Video for Free**, Yang Luo et.al., Paper: [http://arxiv.org/abs/2502.07508](http://arxiv.org/abs/2502.07508)\n", "2410.06241": "- 2025-02-27, **ByTheWay: Boost Your Text-to-Video Generation Model to Higher Quality in a Training-free Way**, Jiazi Bu et.al., Paper: [http://arxiv.org/abs/2410.06241](http://arxiv.org/abs/2410.06241)\n", "2410.13720": "- 2025-02-26, **Movie Gen: A Cast of Media Foundation Models**, Adam Polyak et.al., Paper: [http://arxiv.org/abs/2410.13720](http://arxiv.org/abs/2410.13720)\n", "2501.14195": "- 2025-02-25, **VideoShield: Regulating Diffusion-based Video Generation Models via Watermarking**, Runyi Hu et.al., Paper: [http://arxiv.org/abs/2501.14195](http://arxiv.org/abs/2501.14195)\n", "2502.15672": "- 2025-02-21, **VaViM and VaVAM: Autonomous Driving through Video Generative Modeling**, Florent Bartoccioni et.al., Paper: [http://arxiv.org/abs/2502.15672](http://arxiv.org/abs/2502.15672)\n", "2502.14226": "- 2025-02-20, **Designing Parameter and Compute Efficient Diffusion Transformers using Distillation**, Vignesh Sundaresha et.al., Paper: [http://arxiv.org/abs/2502.14226](http://arxiv.org/abs/2502.14226)\n", "2502.06812": "- 2025-02-17, **Harness Local Rewards for Global Benefits: Effective Text-to-Video Generation Alignment with Patch-level Reward Models**, Shuting Wang et.al., Paper: [http://arxiv.org/abs/2502.06812](http://arxiv.org/abs/2502.06812)\n", "2412.17042": "- 2025-02-17, **Adapting Image-to-Video Diffusion Models for Large-Motion Frame Interpolation**, Luoxu Jin et.al., Paper: [http://arxiv.org/abs/2412.17042](http://arxiv.org/abs/2412.17042)\n", "2502.07701": "- 2025-02-17, **Magic 1-For-1: Generating One Minute Video Clips within One Minute**, Hongwei Yi et.al., Paper: [http://arxiv.org/abs/2502.07701](http://arxiv.org/abs/2502.07701)\n", "2412.08261": "- 2025-02-16, **FLIP: Flow-Centric Generative Planning as General-Purpose Manipulation World Model**, Chongkai Gao et.al., Paper: [http://arxiv.org/abs/2412.08261](http://arxiv.org/abs/2412.08261)\n", "2408.08093": "- 2025-02-14, **When Video Coding Meets Multimodal Large Language Models: A Unified Paradigm for Video Coding**, Pingping Zhang et.al., Paper: [http://arxiv.org/abs/2408.08093](http://arxiv.org/abs/2408.08093)\n", "2502.09268": "- 2025-02-14, **GEVRM: Goal-Expressive Video Generation Model For Robust Visual Manipulation**, Hongyin Zhang et.al., Paper: [http://arxiv.org/abs/2502.09268](http://arxiv.org/abs/2502.09268)\n", "2502.08234": "- 2025-02-12, **Learning Human Skill Generators at Key-Step Levels**, Yilu Wu et.al., Paper: [http://arxiv.org/abs/2502.08234](http://arxiv.org/abs/2502.08234)\n", "2502.07825": "- 2025-02-10, **Pre-Trained Video Generative Models as World Simulators**, Haoran He et.al., Paper: [http://arxiv.org/abs/2502.07825](http://arxiv.org/abs/2502.07825)\n", "2502.04896": "- 2025-02-10, **Goku: Flow Based Video Generative Foundation Models**, Shoufa Chen et.al., Paper: [http://arxiv.org/abs/2502.04896](http://arxiv.org/abs/2502.04896)\n", "2502.05661": "- 2025-02-08, **Towards AI-driven Sign Language Generation with Non-manual Markers**, Han Zhang et.al., Paper: [http://arxiv.org/abs/2502.05661](http://arxiv.org/abs/2502.05661)\n", "2502.04296": "- 2025-02-06, **Learning Real-World Action-Video Dynamics with Heterogeneous Masked Autoregression**, Lirui Wang et.al., Paper: [http://arxiv.org/abs/2502.04296](http://arxiv.org/abs/2502.04296)\n", "2502.04076": "- 2025-02-06, **Content-Rich AIGC Video Quality Assessment via Intricate Text Alignment and Motion-Aware Consistency**, Shangkun Sun et.al., Paper: [http://arxiv.org/abs/2502.04076](http://arxiv.org/abs/2502.04076)\n", "2502.06838": "- 2025-02-06, **TorchResist: Open-Source Differentiable Resist Simulator**, Zixiao Wang et.al., Paper: [http://arxiv.org/abs/2502.06838](http://arxiv.org/abs/2502.06838)\n", "2502.01784": "- 2025-02-03, **VILP: Imitation Learning with Latent Video Planning**, Zhengtong Xu et.al., Paper: [http://arxiv.org/abs/2502.01784](http://arxiv.org/abs/2502.01784)\n", "2501.09982": "- 2025-02-02, **RichSpace: Enriching Text-to-Video Prompt Space via Text Embedding Interpolation**, Yuefan Cao et.al., Paper: [http://arxiv.org/abs/2501.09982](http://arxiv.org/abs/2501.09982)\n", "2410.20158": "- 2025-02-02, **Your Image is Secretly the Last Frame of a Pseudo Video**, Wenlong Chen et.al., Paper: [http://arxiv.org/abs/2410.20158](http://arxiv.org/abs/2410.20158)\n", "2501.13918": "- 2025-01-23, **Improving Video Generation with Human Feedback**, Jie Liu et.al., Paper: [http://arxiv.org/abs/2501.13918](http://arxiv.org/abs/2501.13918)\n", "2501.03006": "- 2025-01-20, **TransPixeler: Advancing Text-to-Video Generation with Transparency**, Luozhou Wang et.al., Paper: [http://arxiv.org/abs/2501.03006](http://arxiv.org/abs/2501.03006)\n", "2501.11340": "- 2025-01-20, **GenVidBench: A Challenging Benchmark for Detecting AI-Generated Video**, Zhenliang Ni et.al., Paper: [http://arxiv.org/abs/2501.11340](http://arxiv.org/abs/2501.11340)\n", "2501.09755": "- 2025-01-16, **Learnings from Scaling Visual Tokenizers for Reconstruction and Generation**, Philippe Hansen-Estruch et.al., Paper: [http://arxiv.org/abs/2501.09755](http://arxiv.org/abs/2501.09755)\n", "2501.07647": "- 2025-01-13, **BlobGEN-Vid: Compositional Text-to-Video Generation with Blob Video Representations**, Weixi Feng et.al., Paper: [http://arxiv.org/abs/2501.07647](http://arxiv.org/abs/2501.07647)\n", "2501.01987": "- 2025-01-10, **Gender Bias in Text-to-Video Generation Models: A case study of Sora**, Mohammad Nadeem et.al., Paper: [http://arxiv.org/abs/2501.01987](http://arxiv.org/abs/2501.01987)\n", "2501.02690": "- 2025-01-05, **GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking**, Weikang Bian et.al., Paper: [http://arxiv.org/abs/2501.02690](http://arxiv.org/abs/2501.02690)\n", "2408.11475": "- 2025-01-05, **TrackGo: A Flexible and Efficient Method for Controllable Video Generation**, Haitao Zhou et.al., Paper: [http://arxiv.org/abs/2408.11475](http://arxiv.org/abs/2408.11475)\n", "2412.00115": "- 2025-01-04, **OpenHumanVid: A Large-Scale High-Quality Dataset for Enhancing Human-Centric Video Generation**, Hui Li et.al., Paper: [http://arxiv.org/abs/2412.00115](http://arxiv.org/abs/2412.00115)\n", "2412.20901": "- 2024-12-30, **ILDiff: Generate Transparent Animated Stickers by Implicit Layout Distillation**, Ting Zhang et.al., Paper: [http://arxiv.org/abs/2412.20901](http://arxiv.org/abs/2412.20901)\n", "2412.20404": "- 2024-12-29, **Open-Sora: Democratizing Efficient Video Production for All**, Zangwei Zheng et.al., Paper: [http://arxiv.org/abs/2412.20404](http://arxiv.org/abs/2412.20404)\n", "2412.19761": "- 2024-12-27, **Generative Video Propagation**, Shaoteng Liu et.al., Paper: [http://arxiv.org/abs/2412.19761](http://arxiv.org/abs/2412.19761)\n", "2408.13423": "- 2024-12-25, **Decoupled Video Generation with Chain of Training-free Diffusion Model Experts**, Wenhao Li et.al., Paper: [http://arxiv.org/abs/2408.13423](http://arxiv.org/abs/2408.13423)\n", "2412.17077": "- 2024-12-22, **SubstationAI: Multimodal Large Model-Based Approaches for Analyzing Substation Equipment Faults**, Jinzhi Wang et.al., Paper: [http://arxiv.org/abs/2412.17077](http://arxiv.org/abs/2412.17077)\n", "2412.16717": "- 2024-12-21, **GANFusion: Feed-Forward Text-to-3D with Diffusion in GAN Space**, Souhaib Attaiki et.al., Paper: [http://arxiv.org/abs/2412.16717](http://arxiv.org/abs/2412.16717)\n", "2412.14484": "- 2024-12-19, **DirectorLLM for Human-Centric Video Generation**, Kunpeng Song et.al., Paper: [http://arxiv.org/abs/2412.14484](http://arxiv.org/abs/2412.14484)\n", "2412.13479": "- 2024-12-18, **Real-time One-Step Diffusion-based Expressive Portrait Videos Generation**, Hanzhong Guo et.al., Paper: [http://arxiv.org/abs/2412.13479](http://arxiv.org/abs/2412.13479)\n", "2412.16211": "- 2024-12-17, **Is Your World Simulator a Good Story Presenter? A Consecutive Events-Based Benchmark for Future Long Video Generation**, Yiping Wang et.al., Paper: [http://arxiv.org/abs/2412.16211](http://arxiv.org/abs/2412.16211)\n", "2412.11755": "- 2024-12-16, **Generative Inbetweening through Frame-wise Conditions-Driven Video Generation**, Tianyi Zhu et.al., Paper: [http://arxiv.org/abs/2412.11755](http://arxiv.org/abs/2412.11755)\n", "2412.09600": "- 2024-12-12, **Owl-1: Omni World Model for Consistent Long Video Generation**, Yuanhui Huang et.al., Paper: [http://arxiv.org/abs/2412.09600](http://arxiv.org/abs/2412.09600)\n", "2412.09389": "- 2024-12-12, **UFO: Enhancing Diffusion-Based Video Generation with a Uniform Frame Organizer**, Delong Liu et.al., Paper: [http://arxiv.org/abs/2412.09389](http://arxiv.org/abs/2412.09389)\n", "2412.07774": "- 2024-12-11, **UniReal: Universal Image Generation and Editing via Learning Real-world Dynamics**, Xi Chen et.al., Paper: [http://arxiv.org/abs/2412.07774](http://arxiv.org/abs/2412.07774)\n", "2412.09647": "- 2024-12-11, **Bench2Drive-R: Turning Real World Data into Reactive Closed-Loop Autonomous Driving Benchmark by Generative Model**, Junqi You et.al., Paper: [http://arxiv.org/abs/2412.09647](http://arxiv.org/abs/2412.09647)\n", "2412.05280": "- 2024-12-11, **Stag-1: Towards Realistic 4D Driving Simulation with Video Generation Model**, Lening Wang et.al., Paper: [http://arxiv.org/abs/2412.05280](http://arxiv.org/abs/2412.05280)\n", "2412.07744": "- 2024-12-10, **StyleMaster: Stylize Your Video with Artistic Generation and Translation**, Zixuan Ye et.al., Paper: [http://arxiv.org/abs/2412.07744](http://arxiv.org/abs/2412.07744)\n", "2412.07730": "- 2024-12-10, **STIV: Scalable Text and Image Conditioned Video Generation**, Zongyu Lin et.al., Paper: [http://arxiv.org/abs/2412.07730](http://arxiv.org/abs/2412.07730)\n", "2412.04623": "- 2024-12-05, **Using Diffusion Priors for Video Amodal Segmentation**, Kaihua Chen et.al., Paper: [http://arxiv.org/abs/2412.04623](http://arxiv.org/abs/2412.04623)\n", "2412.04446": "- 2024-12-05, **DiCoDe: Diffusion-Compressed Deep Tokens for Autoregressive Video Generation with Language Models**, Yizhuo Li et.al., Paper: [http://arxiv.org/abs/2412.04446](http://arxiv.org/abs/2412.04446)\n", "2412.04440": "- 2024-12-05, **GenMAC: Compositional Text-to-Video Generation with Multi-Agent Collaboration**, Kaiyi Huang et.al., Paper: [http://arxiv.org/abs/2412.04440](http://arxiv.org/abs/2412.04440)\n", "2412.03430": "- 2024-12-04, **SINGER: Vivid Audio-driven Singing Video Generation with Multi-scale Spectral Diffusion Model**, Yan Li et.al., Paper: [http://arxiv.org/abs/2412.03430](http://arxiv.org/abs/2412.03430)\n", "2412.02684": "- 2024-12-03, **AniGS: Animatable Gaussian Avatar from a Single Image with Inconsistent Gaussian Reconstruction**, Lingteng Qiu et.al., Paper: [http://arxiv.org/abs/2412.02684](http://arxiv.org/abs/2412.02684), Code: **[https://github.com/aigc3d/AniGS](https://github.com/aigc3d/AniGS)**\n", "2412.00148": "- 2024-11-29, **Motion Modes: What Could Happen Next?**, Karran Pandey et.al., Paper: [http://arxiv.org/abs/2412.00148](http://arxiv.org/abs/2412.00148), Code: **[https://github.com/adobe-research/MotionModes](https://github.com/adobe-research/MotionModes)**\n", "2412.00131": "- 2024-11-28, **Open-Sora Plan: Open-Source Large Video Generation Model**, Bin Lin et.al., Paper: [http://arxiv.org/abs/2412.00131](http://arxiv.org/abs/2412.00131)\n", "2410.13571": "- 2024-11-25, **DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation**, Guosheng Zhao et.al., Paper: [http://arxiv.org/abs/2410.13571](http://arxiv.org/abs/2410.13571)\n", "2411.13609": "- 2024-11-24, **What You See Is What Matters: A Novel Visual and Physics-Based Metric for Evaluating Video Generation Quality**, Zihan Wang et.al., Paper: [http://arxiv.org/abs/2411.13609](http://arxiv.org/abs/2411.13609)\n", "2411.13503": "- 2024-11-20, **VBench++: Comprehensive and Versatile Benchmark Suite for Video Generative Models**, Ziqi Huang et.al., Paper: [http://arxiv.org/abs/2411.13503](http://arxiv.org/abs/2411.13503)\n", "2411.10501": "- 2024-11-15, **OnlyFlow: Optical Flow based Motion Conditioning for Video Diffusion Models**, Mathis Koroglu et.al., Paper: [http://arxiv.org/abs/2411.10501](http://arxiv.org/abs/2411.10501)\n", "2411.09153": "- 2024-11-14, **VidMan: Exploiting Implicit Dynamics from Video Diffusion Model for Effective Robot Manipulation**, Youpeng Wen et.al., Paper: [http://arxiv.org/abs/2411.09153](http://arxiv.org/abs/2411.09153)\n", "2411.07619": "- 2024-11-12, **Artificial Intelligence for Biomedical Video Generation**, Linyuan Li et.al., Paper: [http://arxiv.org/abs/2411.07619](http://arxiv.org/abs/2411.07619)\n", "2411.02914": "- 2024-11-05, **Exploring the Interplay Between Video Generation and World Models in Autonomous Driving: A Survey**, Ao Fu et.al., Paper: [http://arxiv.org/abs/2411.02914](http://arxiv.org/abs/2411.02914)\n", "2411.01647": "- 2024-11-03, **Optical Flow Representation Alignment Mamba Diffusion Model for Medical Video Generation**, Zhenbin Wang et.al., Paper: [http://arxiv.org/abs/2411.01647](http://arxiv.org/abs/2411.01647)\n", "2410.23277": "- 2024-10-31, **SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video Generation**, Yining Hong et.al., Paper: [http://arxiv.org/abs/2410.23277](http://arxiv.org/abs/2410.23277)\n", "2410.23836": "- 2024-10-31, **Stereo-Talker: Audio-driven 3D Human Synthesis with Prior-Guided Mixture-of-Experts**, Xiang Deng et.al., Paper: [http://arxiv.org/abs/2410.23836](http://arxiv.org/abs/2410.23836)\n", "2410.20018": "- 2024-10-26, **GHIL-Glue: Hierarchical Control with Filtered Subgoal Images**, Kyle B. Hatch et.al., Paper: [http://arxiv.org/abs/2410.20018](http://arxiv.org/abs/2410.20018)\n", "2410.18072": "- 2024-10-23, **WorldSimBench: Towards Video Generation Models as World Simulators**, Yiran Qin et.al., Paper: [http://arxiv.org/abs/2410.18072](http://arxiv.org/abs/2410.18072)\n", "2410.15458": "- 2024-10-20, **Allegro: Open the Black Box of Commercial-Level Video Generation Model**, Yuan Zhou et.al., Paper: [http://arxiv.org/abs/2410.15458](http://arxiv.org/abs/2410.15458)\n", "2410.08534": "- 2024-10-20, **Quality Prediction of AI Generated Images and Videos: Emerging Trends and Opportunities**, Abhijay Ghildyal et.al., Paper: [http://arxiv.org/abs/2410.08534](http://arxiv.org/abs/2410.08534)\n", "2410.10816": "- 2024-10-14, **LVD-2M: A Long-take Video Dataset with Temporally Dense Captions**, Tianwei Xiong et.al., Paper: [http://arxiv.org/abs/2410.10816](http://arxiv.org/abs/2410.10816)\n", "2410.10751": "- 2024-10-14, **DragEntity: Trajectory Guided Video Generation using Entity and Positional Relationships**, Zhang Wan et.al., Paper: [http://arxiv.org/abs/2410.10751](http://arxiv.org/abs/2410.10751)\n", "2410.03143": "- 2024-10-12, **ECHOPulse: ECG controlled echocardio-grams video generation**, Yiwei Li et.al., Paper: [http://arxiv.org/abs/2410.03143](http://arxiv.org/abs/2410.03143)\n", "2410.05677": "- 2024-10-11, **T2V-Turbo-v2: Enhancing Video Generation Model Post-Training through Data, Reward, and Conditional Guidance Design**, Jiachen Li et.al., Paper: [http://arxiv.org/abs/2410.05677](http://arxiv.org/abs/2410.05677)\n", "2410.14715": "- 2024-10-10, **Animating the Past: Reconstruct Trilobite via Video Generation**, Xiaoran Wu et.al., Paper: [http://arxiv.org/abs/2410.14715](http://arxiv.org/abs/2410.14715)\n", "2410.00262": "- 2024-09-30, **ImmersePro: End-to-End Stereo Video Synthesis Via Implicit Disparity Learning**, Jian Shi et.al., Paper: [http://arxiv.org/abs/2410.00262](http://arxiv.org/abs/2410.00262)\n", "2408.14028": "- 2024-09-24, **SurGen: Text-Guided Diffusion Model for Surgical Video Generation**, Joseph Cho et.al., Paper: [http://arxiv.org/abs/2408.14028](http://arxiv.org/abs/2408.14028)\n", "2409.16283": "- 2024-09-24, **Gen2Act: Human Video Generation in Novel Scenarios enables Generalizable Robot Manipulation**, Homanga Bharadhwaj et.al., Paper: [http://arxiv.org/abs/2409.16283](http://arxiv.org/abs/2409.16283)\n", "2410.07194": "- 2024-09-24, **Technical Report: Competition Solution For Modelscope-Sora**, Shengfu Chen et.al., Paper: [http://arxiv.org/abs/2410.07194](http://arxiv.org/abs/2410.07194)\n", "2409.02657": "- 2024-09-04, **PoseTalk: Text-and-Audio-based Pose Control and Motion Refinement for One-Shot Talking Head Generation**, Jun Ling et.al., Paper: [http://arxiv.org/abs/2409.02657](http://arxiv.org/abs/2409.02657)\n", "2408.15868": "- 2024-08-28, **GenDDS: Generating Diverse Driving Video Scenarios with Prompt-to-Video Generative Model**, Yongjie Fu et.al., Paper: [http://arxiv.org/abs/2408.15868](http://arxiv.org/abs/2408.15868)\n", "2408.11788": "- 2024-08-21, **DreamFactory: Pioneering Multi-Scene Long Video Generation with a Multi-Agent Framework**, Zhifei Xie et.al., Paper: [http://arxiv.org/abs/2408.11788](http://arxiv.org/abs/2408.11788)\n"}}
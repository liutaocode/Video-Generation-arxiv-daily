#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import re
import json
import arxiv
import yaml
import logging
import datetime
import requests
from typing import Dict, List, Tuple, Optional
import time

logging.basicConfig(format='[%(asctime)s %(levelname)s] %(message)s',
                    datefmt='%m/%d/%Y %H:%M:%S',
                    level=logging.INFO)

# Define global variables               
base_url = "https://arxiv.paperswithcode.com/api/v0/papers/"
github_url = "https://api.github.com/search/repositories"
arxiv_url = "http://arxiv.org/"

def load_config(config_file:str) -> dict:
    '''
    config_file: input config file path
    return: a dict of configuration
    '''
    # make filters pretty
    def pretty_filters(**config) -> dict:
        keywords = dict()
        EXCAPE = '\"'
        QUOTA = '' # NO-USE
        OR = 'OR' # TODO
        def parse_filters(filters:list):
            ret = ''
            for idx in range(0,len(filters)):
                filter = filters[idx]
                if len(filter.split()) > 1:
                    ret += (EXCAPE + filter + EXCAPE)  
                else:
                    ret += (QUOTA + filter + QUOTA)   
                if idx != len(filters) - 1:
                    ret += OR
            return ret
        for k,v in config['keywords'].items():
            keywords[k] = parse_filters(v['filters'])
        return keywords
    with open(config_file,'r') as f:
        config = yaml.load(f,Loader=yaml.FullLoader) 
        config['kv'] = pretty_filters(**config)
        logging.info(f'config = {config}')
    return config 

def get_authors(authors, first_author = False):
    output = str()
    if first_author == False:
        output = ", ".join(str(author) for author in authors)
    else:
        output = authors[0]
    return output

def get_code_link(qword:str) -> str:
    """
    This short function was auto-generated by ChatGPT. 
    I only renamed some params and added some comments.
    @param qword: query searching word
    @return paper_code in github: string, if not found return None
    """
    try:
        # Search for repositories on GitHub
        params = {
            "q": qword,
            "sort": "stars",
            "order": "desc"
        }
        r = requests.get(github_url, params=params)
        results = r.json()
        code_link = None
        if "total_count" in results and results["total_count"] > 0:
            code_link = results["items"][0]["html_url"]
        return code_link
    except Exception as e:
        logging.debug(f"GitHub search failed for {qword}: {e}")
        return None

def fetch_papers_in_batches(query: str, total_papers: int, batch_size: int = 50) -> List[arxiv.Result]:
    """
    Fetch papers using multiple independent searches with date filtering
    to avoid pagination issues.
    """
    all_results = []
    seen_ids = set()
    
    # Start from today and work backwards
    end_date = datetime.datetime.now()
    
    # We'll fetch papers day by day or week by week until we have enough
    days_back = 0
    max_days = 365  # Look back at most 1 year
    
    while len(all_results) < total_papers and days_back < max_days:
        # Calculate date range for this batch
        start_date = end_date - datetime.timedelta(days=7)  # Fetch week by week
        
        # Format dates for arxiv query
        start_str = start_date.strftime("%Y%m%d")
        end_str = end_date.strftime("%Y%m%d")
        
        # Add date filter to query
        date_query = f"{query} AND submittedDate:[{start_str} TO {end_str}]"
        
        logging.info(f"Fetching papers from {start_str} to {end_str}")
        
        client = arxiv.Client()
        search = arxiv.Search(
            query=date_query,
            max_results=min(batch_size, total_papers - len(all_results)),
            sort_by=arxiv.SortCriterion.SubmittedDate
        )
        
        try:
            batch_results = []
            for result in client.results(search):
                paper_id = result.get_short_id()
                if paper_id not in seen_ids:
                    seen_ids.add(paper_id)
                    batch_results.append(result)
                    
            if batch_results:
                all_results.extend(batch_results)
                logging.info(f"Fetched {len(batch_results)} new papers (total: {len(all_results)})")
            else:
                logging.info(f"No new papers found in date range")
                
        except arxiv.UnexpectedEmptyPageError:
            logging.warning(f"Empty page for date range {start_str} to {end_str}")
        except Exception as e:
            logging.error(f"Error fetching papers for date range: {e}")
        
        # Move to the previous week
        end_date = start_date
        days_back += 7
        
        # Small delay to avoid rate limiting
        time.sleep(3)
    
    # Sort all results by date (newest first)
    all_results.sort(key=lambda x: x.updated, reverse=True)
    
    # Return only the requested number of papers
    return all_results[:total_papers]

def get_daily_papers(topic, query="slam", max_results=2):
    """
    @param topic: str
    @param query: str
    @return paper_with_code: dict
    """
    # output 
    content = dict() 
    content_to_web = dict()
    
    logging.info(f"Starting paper collection for topic: {topic}")
    logging.info(f"Query: {query[:100]}...")
    logging.info(f"Target: {max_results} papers")
    
    # Fetch papers using the robust batch method
    results = fetch_papers_in_batches(query, max_results, batch_size=50)
    
    logging.info(f"Processing {len(results)} fetched papers")
    
    for idx, result in enumerate(results, 1):
        try:
            paper_id = result.get_short_id()
            paper_title = result.title
            paper_url = result.entry_id
            paper_abstract = result.summary.replace("\n", " ")
            paper_authors = get_authors(result.authors)
            paper_first_author = get_authors(result.authors, first_author=True)
            primary_category = result.primary_category
            publish_time = result.published.date()
            update_time = result.updated.date()
            comments = result.comment
            
            logging.info(f"[{idx}/{len(results)}] Time = {update_time} title = {paper_title} author = {paper_first_author}")
            
            # eg: 2108.09112v1 -> 2108.09112
            ver_pos = paper_id.find('v')
            if ver_pos == -1:
                paper_key = paper_id
            else:
                paper_key = paper_id[0:ver_pos]
            paper_url = arxiv_url + 'abs/' + paper_key
            
            # Try to find code link from GitHub search
            repo_url = get_code_link(paper_title)
            if repo_url is None:
                repo_url = get_code_link(paper_key)
            
            if repo_url is not None:
                content[paper_key] = "|**{}**|**{}**|{} et.al.|[{}]({})|**[link]({})**|\n".format(
                    update_time, paper_title, paper_first_author, paper_key, paper_url, repo_url)
                content_to_web[paper_key] = "- {}, **{}**, {} et.al., Paper: [{}]({}), Code: **[{}]({})**".format(
                    update_time, paper_title, paper_first_author, paper_url, paper_url, repo_url, repo_url)
            else:
                content[paper_key] = "|**{}**|**{}**|{} et.al.|[{}]({})|null|\n".format(
                    update_time, paper_title, paper_first_author, paper_key, paper_url)
                content_to_web[paper_key] = "- {}, **{}**, {} et.al., Paper: [{}]({})".format(
                    update_time, paper_title, paper_first_author, paper_url, paper_url)
            
            # TODO: select useful comments
            comments = None
            if comments != None:
                content_to_web[paper_key] += f", {comments}\n"
            else:
                content_to_web[paper_key] += f"\n"
                
        except Exception as e:
            logging.error(f"Error processing paper {idx}: {e}")
            continue
    
    logging.info(f"Successfully processed {len(content)} papers")
    
    data = {topic: content}
    data_web = {topic: content_to_web}
    return data, data_web

def update_paper_links(filename):
    '''
    weekly update paper links in json file 
    '''
    def parse_arxiv_string(s):
        parts = s.split("|")
        date = parts[1].strip()
        title = parts[2].strip()
        authors = parts[3].strip()
        arxiv_id = parts[4].strip()
        code = parts[5].strip()
        arxiv_id = re.sub(r'v\d+', '', arxiv_id)
        return date,title,authors,arxiv_id,code

    with open(filename,"r") as f:
        content = f.read()
        if not content:
            m = {}
        else:
            m = json.loads(content)
            
        json_data = m.copy() 

        for keywords,v in json_data.items():
            logging.info(f'keywords = {keywords}')
            for paper_id,contents in v.items():
                contents = str(contents)

                update_time, paper_title, paper_first_author, paper_url, code_url = parse_arxiv_string(contents)

                contents = "|{}|{}|{}|{}|{}|\n".format(update_time,paper_title,paper_first_author,paper_url,code_url)
                json_data[keywords][paper_id] = str(contents)
                logging.info(f'paper_id = {paper_id}, contents = {contents}')
                
                valid_link = False if '|null|' in contents else True
                if valid_link:
                    continue
                
                # Update code link
                repo_url = get_code_link(paper_title)
                if repo_url is None:
                    repo_url = get_code_link(paper_id)
                    
                if repo_url is not None:
                    contents = "|{}|{}|{}|{}|**[link]({})**|\n".format(update_time,paper_title,paper_first_author,paper_url,repo_url)
                    json_data[keywords][paper_id] = str(contents)
                    logging.info(f'paper_id = {paper_id}, contents = {contents}')
                    
    with open(filename,"w") as f:
        json.dump(json_data,f)
        
def json_to_md(filename, md_filename,
               to_web=False, 
               use_title=True, 
               use_tc=True,
               show_badge=True,
               use_emoji=True):
    """
    @param filename: str
    @param md_filename: str
    @return None
    """
    
    def pretty_math(s:str) -> str:
        ret = ''
        match = re.search(r'\$.*\$', s)
        if match == None:
            return s
        math_start,math_end = match.span()
        space_trail = space_leading = ''
        if s[:math_start][-1] != ' ' and '*' != s[:math_start][-1]: space_leading = ' ' 
        if s[math_end:][0] != ' ' and '*' != s[math_end:][0]: space_trail = ' ' 
        ret = s[:math_start] + space_leading + match.group(0) + space_trail + s[math_end:]
        return ret
    
    DateNow = datetime.date.today()
    DateNow = str(DateNow)
    DateNow = DateNow.replace('-','.')
    
    with open(filename,"r") as f:
        content = f.read()
        if not content:
            data = {}
        else:
            data = json.loads(content)

    # clean README.md if daily already exist else create it
    with open(md_filename,"w+") as f:
        pass

    # write data into README.md
    with open(md_filename,"a+") as f:

        if (use_title == True) and (to_web == True):
            f.write("---\n" + "layout: default\n" + "---\n\n")
        
        if show_badge == True:
            f.write(f"[![Contributors][contributors-shield]][contributors-url]\n")
            f.write(f"[![Forks][forks-shield]][forks-url]\n")
            f.write(f"[![Stargazers][stars-shield]][stars-url]\n")
            f.write(f"[![Issues][issues-shield]][issues-url]\n\n")    
                
        if use_title == True:
            # TODO: add robot pic
            f.write("## Updated on " + DateNow + "\n")
        elif use_tc == True:
            f.write("## Table of contents \n")
            
        if use_emoji == True:
            # f.write("> Updated on " + DateNow + "\n")
            f.write("> Welcome to Video Generation papers! \n\n")
            
        for keyword in data.keys():
            day_content = data[keyword]
            if not day_content:
                continue
            # the head of each part
            f.write(f"## {keyword}\n\n")

            if use_title == True :
                if to_web == False:
                    f.write("|Publish Date|Title|Authors|PDF|Code|\n" + "|---|---|---|---|---|\n")
                else:
                    f.write("| Publish Date | Title | Authors | PDF | Code |\n")
                    f.write("|:---------|:-----------------------|:---------|:------|:------|\n")

            # sort papers by date
            day_content = sort_papers(day_content)
            
            for _,v in day_content.items():
                if v is not None:
                    f.write(pretty_math(v))

            f.write(f"\n")

        # TODO: use shields.io
        if show_badge == True:
            # TODO: add <p align="center"> </p> for center
            f.write(f"[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge\n")
            f.write(f"[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors\n")
            f.write(f"[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge\n")
            f.write(f"[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members\n")
            f.write(f"[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge\n")
            f.write(f"[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers\n")
            f.write(f"[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge\n")
            f.write(f"[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues\n\n")
                
    logging.info(f"Finished writing {md_filename}")

def json_to_md_web(filename, md_filename):
    with open(filename, "r") as f:
        content = f.read()
        if not content:
            data = {}
        else:
            data = json.loads(content)

    # Clean and create file
    with open(md_filename, "w+") as f:
        pass

    # Write data
    with open(md_filename, "a+") as f:
        f.write("---\n" + "layout: default\n" + "---\n\n")
        
        DateNow = datetime.date.today()
        DateNow = str(DateNow)
        DateNow = DateNow.replace('-', '.')
        f.write("## Updated on " + DateNow + "\n\n")
        f.write("> Welcome to Video Generation papers! \n\n")
        
        for keyword in data.keys():
            day_content = data[keyword]
            if not day_content:
                continue
            
            f.write(f"## {keyword}\n\n")
            
            # Sort papers by date
            day_content = sort_papers(day_content)
            
            for _, v in day_content.items():
                if v is not None:
                    f.write(v)
            
            f.write(f"\n")
    
    logging.info(f"Finished writing {md_filename}")

def sort_papers(papers):
    output = dict()
    keys = list(papers.keys())
    keys.sort(reverse=True)
    for key in keys:
        output[key] = papers[key]
    return output    

def get_daily_papers_fixed(keywords, max_results=2):
    """
    Main function to get daily papers
    """
    data_collector = {}
    data_collector_web = {}
    
    for topic, keyword in keywords.items():
        logging.info(f"Processing topic: {topic}")
        data, data_web = get_daily_papers(topic, query=keyword, max_results=max_results)
        data_collector.update(data)
        data_collector_web.update(data_web)
        
    return data_collector, data_collector_web

def update_json_file(filename, data_collector):
    """
    Update json file with new data
    """
    if os.path.exists(filename):
        with open(filename, "r") as f:
            content = f.read()
            if content:
                m = json.loads(content)
            else:
                m = {}
    else:
        m = {}
    
    # Merge new data with existing data
    for topic in data_collector:
        if topic in m:
            m[topic].update(data_collector[topic])
        else:
            m[topic] = data_collector[topic]
    
    with open(filename, "w") as f:
        json.dump(m, f)

def main():
    # Load config
    config = load_config("config.yaml")
    keywords = config['kv']
    max_results = config['max_results']
    publish_readme = config['publish_readme']
    publish_gitpage = config['publish_gitpage']
    publish_wechat = config['publish_wechat']
    
    # Collect papers
    logging.info("GET daily papers begin")
    data_collector, data_collector_web = get_daily_papers_fixed(keywords, max_results)
    logging.info("GET daily papers end")
    
    # Update files
    update_json_file("docs/vg-arxiv-daily.json", data_collector)
    update_json_file("docs/vg-arxiv-daily-web.json", data_collector_web)
    
    # Update paper links
    if config['update_paper_links']:
        update_paper_links("docs/vg-arxiv-daily.json")
        update_paper_links("docs/vg-arxiv-daily-web.json")
    
    # Generate README
    if publish_readme:
        json_to_md("docs/vg-arxiv-daily.json", "README.md", to_web=False)
        logging.info("Update Readme finished")
    
    # Generate GitPage
    if publish_gitpage:
        json_to_md_web("docs/vg-arxiv-daily-web.json", "docs/index.md")
        logging.info("Update GitPage finished")

if __name__ == "__main__":
    main()